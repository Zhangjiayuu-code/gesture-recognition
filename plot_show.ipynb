{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import board\n",
    "import adafruit_icm20x\n",
    "from adafruit_tca9548a import TCA9548A\n",
    "from quarternion import Quaternion\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(24,24),nn.ReLU(),\n",
    "            nn.Linear(24,14),nn.ReLU(),\n",
    "            nn.Linear(14,8)\n",
    "        )\n",
    "        # 初始化模型的权重和偏置为 Float 类型\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data = layer.weight.data.double()\n",
    "                layer.bias.data = layer.bias.data.double()\n",
    "                \n",
    "    def forward (self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model_path = './MLP_model .ckpt'\n",
    "\n",
    "\n",
    "model=MLP()\n",
    "model=model.to('cpu')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Initialize I2C bus\n",
    "i2c = board.I2C()\n",
    "# Initialize PCA9548A mux\n",
    "mux = TCA9548A(i2c)\n",
    "\n",
    "muzhi = adafruit_icm20x.ICM20948(mux[0], address=0x68)\n",
    "shizhi = adafruit_icm20x.ICM20948(mux[1], address=0x68)\n",
    "zhongzhi = adafruit_icm20x.ICM20948(mux[2], address=0x68)\n",
    "wumingzhi = adafruit_icm20x.ICM20948(mux[3], address=0x68)\n",
    "xiaomuzhi = adafruit_icm20x.ICM20948(mux[4], address=0x68)\n",
    "shoubei = adafruit_icm20x.ICM20948(mux[5], address=0x68)\n",
    "\n",
    "qua=Quaternion(muzhi,shizhi,zhongzhi,wumingzhi,xiaomuzhi,shoubei)\n",
    "\n",
    "# {0: 'call', 1: 'five', 2: 'love', 3: 'ok', 4: 'one', 5: 'rock', 6: 'yes', 7: 'zero'}\n",
    "\n",
    "gesture_images = {\n",
    "    0: './gesture/call.png',\n",
    "    1: './gesture/five.png',\n",
    "    2: './gesture/love.png',\n",
    "    3: './gesture/ok.png',\n",
    "    4: './gesture/one.png',\n",
    "    5: './gesture/rock.png',\n",
    "    6: './gesture/yes.png',\n",
    "    7: './gesture/zero.png'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: './gesture/call.png', 1: './gesture/five.png', 2: './gesture/love.png', 3: './gesture/ok.png', 4: './gesture/one.png', 5: './gesture/rock.png', 6: './gesture/yes.png', 7: './gesture/zero.png'}\n"
     ]
    }
   ],
   "source": [
    "print(gesture_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gesture: 6\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/FYP/torch/lib/python3.11/site-packages/PIL/Image.py:3251\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3251\u001b[0m     fp\u001b[39m.\u001b[39;49mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m   3252\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pi/FYP/final_vertion/plot_show.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pi/FYP/final_vertion/plot_show.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image_path \u001b[39m=\u001b[39m gesture_images\u001b[39m.\u001b[39mget(label)  \u001b[39m# 如果手势标签对应的图片不存在，显示默认图片\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pi/FYP/final_vertion/plot_show.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(image_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pi/FYP/final_vertion/plot_show.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pi/FYP/final_vertion/plot_show.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(\u001b[39m\"\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pi/FYP/final_vertion/plot_show.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(image)\n",
      "File \u001b[0;32m~/FYP/torch/lib/python3.11/site-packages/PIL/Image.py:3253\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3251\u001b[0m     fp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m   3252\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n\u001b[0;32m-> 3253\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m   3254\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(\u001b[39m16\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #return a dict\n",
    "    dic=qua.datareader()\n",
    "    sensor_data = [\n",
    "        dic[\"muzhi\"].tolist()+dic[\"shizhi\"].tolist()+dic[\"zhongzhi\"].tolist()+\n",
    "        dic[\"wumingzhi\"].tolist()+dic[\"xiaomuzhi\"].tolist()+dic[\"shoubei\"].tolist()\n",
    "    ]\n",
    "    sensor_data=torch.tensor(sensor_data,dtype=torch.double)\n",
    "    output=model(sensor_data)\n",
    "\n",
    "    # {0: 'call', 1: 'five', 2: 'love', 3: 'ok', 4: 'one', 5: 'rock', 6: 'yes', 7: 'zero'}\n",
    "    label=output.argmax(dim=-1)\n",
    "    \n",
    "    print(\"Predicted Gesture:\", label.item())\n",
    "    image_path = gesture_images.get(label)  # 如果手势标签对应的图片不存在，显示默认图片\n",
    "    print(image_path)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    plt.figure(\"Image\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('on')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1.5)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
